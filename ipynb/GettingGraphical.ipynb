{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Graphical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces graphical output for text processing. It's part of the [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) and assumes that you've already worked through previous notebooks ([Getting Setup](GettingSetup.ipynb), [Getting Started](GettingStarted.ipynb),  [Getting Texts](GettingTexts.ipynb) and [Getting NLTK](GettingNltk.ipynb)). In this notebook we'll look in particular at\n",
    "\n",
    "* installing the matlablib library (for producing visualizations)\n",
    "* plotting high frequency terms\n",
    "* plotting a characteristic curve of word lengths\n",
    "* plotting a distribution graph of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Matlablib Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ENGL345 we installed Anaconda during the intial setup, so the Matplotlib library we'll use for drawing charts should already be present.\n",
    "\n",
    "We can test this in a new notebook cell to make sure that everything is working. The syntax below also shows how we can create a shorthand name for a library so that instead of always writing ```matplotlib.pyplot``` we can simply write ```plt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # imports a module from the matplotlib library and names it 'plt'\n",
    "\n",
    "# make sure that graphs are embedded into our notebook output\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot() # create an empty graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous notebook on [Getting NLTK](GettingNltk.ipynb) explained the basics of tokenization, filtering, listing frequencies and concordances. This notebook assumes that you have completed Getting NLTK and have a copy of 'goldBug.txt' saved in a directory called data in your working folder. \n",
    "\n",
    "You can check this in the Finder before proceeding, or run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('data/goldBug.txt'):\n",
    "    print(\"We have the Gold-Bug text.\")\n",
    "else:\n",
    "    print(\"We don't have it. Go back to Getting NLTK to save a copy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick up where we left off by (re)reading our _Gold Bug_ text, tokenizing, filtering to keep only words, calculating frequencies and showing a table of the top frequency words. We'd previously created a filtered list that removed stopwords (very common syntactic words that don't carry much meaning), but for now we'll keep all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# read Gold Bug plain text into string\n",
    "f = open(\"data/goldBug.txt\", \"r\")\n",
    "goldBugString = f.read()\n",
    "f.close()\n",
    "\n",
    "# simple lowercase tokenize\n",
    "goldBugTokensLowercase = nltk.word_tokenize(goldBugString.lower())\n",
    "\n",
    "# filter out tokens that aren't words\n",
    "goldBugWordTokensLowercase = [word for word in goldBugTokensLowercase if any(c.isalpha() for c in word)]\n",
    "\n",
    "# determine frequencies\n",
    "goldBugWordTokensLowercaseFreqs = nltk.FreqDist(goldBugWordTokensLowercase)\n",
    "\n",
    "# preview the top 25 frequencies\n",
    "goldBugWordTokensLowercaseFreqs.tabulate(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is useful for ranking the top frequency terms (from left to right), though it's difficult to get a sense from the numbers of how the frequencies compare. Do the numbers drop gradually, precipitously or irregularly? This is a perfect scenario for experimenting with visualization by producing a simple graph.\n",
    "\n",
    "In addition to the ```tabulate()``` function, the frequencies (FreqDist) object that we created as a ```plot()``` function that conveniently plots a graph of the top frequency terms. In order to embed a graph in the output of an iPython Notebook we need to give the following special instruction: ```%matplotlib inline```. This is a little piece of magic specific to iPython notebooks. Like an ```import``` statement, we just need to do this once for the first cell in the notebook where it's relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure that graphs are embedded into our notebook output\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the top frequency words in a graph\n",
    "goldBugWordTokensLowercaseFreqs.plot(25, title=\"Top Frequency Word Tokens in Gold Bug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows not only the rank of the words (along the bottom x axis), but is also much more effective than the table at showing the steep decline in frequncy as we move away from the first words. This is actually a well-known phenomenon with natural language and is described by Zipf's law, which the [Wikipedia article](http://en.wikipedia.org/wiki/Zipf's_law) nicely summarizes:\n",
    "\n",
    "> Zipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc. \n",
    "\n",
    "As we continue to explore frequency of words, it's useful to keep in mind the distinction between frequency rank and the actual number of words (tokens) that each word form (type) is contributing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Characteristic Curve of Word Lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first examples we have of quantitative stylistics (text analysis) is an 1887 study by T.C. Mendenhall who manually counted the length of words and used that to suggest that authors had a distinctive stylistic signature, based on the average word length of their writings. In some ways this is similar to the type/token ratio we saw in the previous notebook, as it tries to measure stylistic features of texts without considering (yet) what the words may mean. It also uses all words, even the function words that authors are maybe using less deliberately. Unlike with the type/token ratios, Mendenhall's Characteristic Curve is less sensitive to changes in the total text length. If an author uses relatively longer words, chances are that style will persist throughout a text (which is different from comparing type/token ratios for a text of 1,000 words or 100,000 words).\n",
    "\n",
    "To calculate the frequencies of terms, we can start by replacing each word in our tokens list with the length of that word. So, instead of this:\n",
    "\n",
    "```python\n",
    "[word for word in tokens]```\n",
    "\n",
    "we have this:\n",
    "\n",
    "```python\n",
    "[len(word) for word in tokens]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goldBugLowerCaseWordTokenLengths = [len(w) for w in goldBugWordTokensLowercase]\n",
    "print(\"first five words: \", goldBugWordTokensLowercase[:5])\n",
    "print(\"first five word lengths: \", goldBugLowerCaseWordTokenLengths[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks right, \"the\" is 3 letters, \"gold-bug\" is 8, etc.\n",
    "\n",
    "Now, just as we counted the frequencies of repeating words, we can count the frequencies of repeating word lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.FreqDist(goldBugLowerCaseWordTokenLengths).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, but not really what we want, since the word lengths on the bottom axis are ordered by frequency (3 is the most common word length, followed by 2, and then 4). The default behaviour of ordering by frequency was useful for words, but not as useful here if we want to order by word length.\n",
    "\n",
    "To accomplish what we want, we'll extract items from the frequency list, which provides a sorting by key (by word length), and then create a list from that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goldBugLowerCaseWordTokenLengthFreqs = list(nltk.FreqDist(goldBugLowerCaseWordTokenLengths).items())\n",
    "goldBugLowerCaseWordTokenLengthFreqs # sorted by word length (not frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, this is a list of tuples where each line represents an item in the list and within each line item there's a fixed-order tuple of two numbers, the first for the word length and the second for the frequency. Since lists don't have a built-in ```plot()``` function – unlike FreqDist that we used previously to plot high frequency words – we need to call the graphing library directly and plot the x (word lengths) and y (frequencies). \n",
    "\n",
    "Read the code below, and answer the question given in the commented lines (recall a comment starts with a '#' character, and isnt' executed by the computer. It allows you to add further explanations and documentation to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the list of pairs ('tuples') above to redefine our word length list\n",
    "goldBugLowerCaseWordTokenWordLengths = [f[0] for f in goldBugLowerCaseWordTokenLengthFreqs]\n",
    "goldBugLowerCaseWordTokenWordLengthValues = [f[1] for f in goldBugLowerCaseWordTokenLengthFreqs]\n",
    "# Complete the following: for words 5 characters long,\n",
    "# f[0] = ?\n",
    "# f[1] = ?\n",
    "\n",
    "plt.plot(goldBugLowerCaseWordTokenWordLengths, goldBugLowerCaseWordTokenWordLengthValues)\n",
    "plt.xlabel('Word Length')\n",
    "plt.ylabel('Word Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty darn close to what some of Mendenhall's graphs looked like, such as this one for the first thousand words of _Oliver Twist_:\n",
    "\n",
    "![Characteristic Curve](images/characteristic-curve-mendenhall.png)\n",
    "\n",
    "Thank goodness we didn't need to count tens of thousands of tokens by hand (an error-prone process) like Mendenhall did!\n",
    "\n",
    "On its own one characteristic curve isn't terribly useful since the point is to compare an author's curve with another, but for now at least we know we can fairly easily generate the output for one text. For now, let's shift back to working with words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous notebooks, sometimes it's useful to work with all word tokens (like when measuring Zipf's Law or aggregate word length, but typically we need to strip out function words to start studying the meaning of texts. Let's recapitulate the filtering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"English\")\n",
    "goldBugContentWordTokensLowercase = [word for word in goldBugWordTokensLowercase if word not in stopwords]\n",
    "goldBugContentWordTokensLowercaseFreqs = nltk.FreqDist(goldBugContentWordTokensLowercase)\n",
    "goldBugContentWordTokensLowercaseFreqs.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, now that we've done some plotting, we could graph the top frequency content terms, though it may be harder to read the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goldBugContentWordTokensLowercaseFreqs.plot(20, title=\"Top Frequency Content Terms in Gold Bug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'd noticed in the last notebook, the words \"jupiter\" and \"legrand\" are suspiciously high frequency (for relatively uncommon words), which may suggest that they're being used as character names in the story. We can regenerate our NLTK text object and ask for concordances of each to confirm this hypothesis. To help differentiate between upper and lowercase words, we'll re-tokenize the text and not perform any case alternation or filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goldBugText = nltk.Text(nltk.word_tokenize(goldBugString))\n",
    "goldBugText.concordance(\"jupiter\", lines=5)\n",
    "goldBugText.concordance(\"legrand\", lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these two character names present throughout the story? One way to get a quick sense is to create a dispersion plot, which is essentially a distribution graph of occurrences. Note that [dispersion_plot()](http://www.nltk.org/api/nltk.html?highlight=dispersion_plot#nltk.text.Text.dispersion_plot) takes a list of words as an argument, but that the words are case-sensitive (unlike the ```concordance()``` function). Since case matters, for other purposes it might have been preferable to use the lowercase tokens instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "goldBugText.dispersion_plot([\"Jupiter\", \"Legrand\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph suggests that there are many more occurrences of the character names in the first half of the text. This doesn't necessarily mean that the characters are not as present in the second half, but their names appear less often (perhaps because of a shift in dialogue structure or narrative focus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some tasks to try:\n",
    "\n",
    "* generate a simple list of the top 20 frequency lowercase content terms (without counts, just the terms)\n",
    "* create a dispersion plot of these terms, do any other stand out as irregularly distributed?\n",
    "* try the command [goldBugText.collocations()](http://www.nltk.org/api/nltk.html?highlight=text#nltk.text.Text.collocations) – what does this do? how might it be useful?\n",
    "\n",
    "In the next notebook we're going to look at more powerful ways of matching terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "From [The Art of Literary Text Analysis](ArtOfLiteraryTextAnalysis.ipynb) by [Stéfan Sinclair](http://stefansinclair.name) &amp; [Geoffrey Rockwell](http://geoffreyrockwell.com), [CC BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)<br />\n",
    "Created January 27, 2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
